{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of both the methods i.e. Using Mathematical and Gradient Descent Approches are given in here [Linear Regression](http://localhost:8888/lab/tree/Regression%20Implementation.ipynb)\n",
    "\n",
    "### The math and concepts behind them are explained in detail below:\n",
    "* Mathematical Approach\n",
    "* Gradient Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression (Mathematical Formulation)\n",
    "\n",
    "---\n",
    "\n",
    "$ y = \\beta _{0} + X_{1} \\beta _{1} + X_{2} \\beta _{2} + X_{3} \\beta _{3} + \\cdots + X_{n} \\beta _{n} $\n",
    "\n",
    "<b>--> step 1</b>: Representation of the above equation\n",
    "\n",
    "$ Y = X  \\beta + \\beta_{0} $\n",
    "\n",
    "where the $\\beta_{0}$ can be merged into $\\beta$ where X can be converted to <br>\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "a_{0} & a_{01} & \\cdots & a_{0(n-1)} & a_{0n}\\\\\n",
    "&\\ddots&\\\\\n",
    "&&\\ddots&\\\\\n",
    "&&&\\ddots&\\\\\n",
    "a_{n} & a_{n1} & \\cdots & a_{n(n-1)} & a_{nn}\n",
    "\\end{bmatrix} \\to\n",
    "\\begin{bmatrix}\n",
    "a_{0} & a_{01} & \\cdots & a_{0(n-1)} & a_{0n} & 1\\\\\n",
    "&\\ddots&&&&1\\\\\n",
    "&&\\ddots&&&1\\\\\n",
    "&&&\\ddots&&\\vdots\\\\\n",
    "a_{n} & a_{n1} & \\cdots & a_{n(n-1)} & a_{nn} & 1\n",
    "\\end{bmatrix}\\text{ where the dimensions change from n}\\times\\text{p}\\to\\text{n}\\times\\text{(p+1)}\n",
    "$\n",
    "\n",
    "and $\\beta$ becomes\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "\\beta_{1}\\\\\n",
    "\\beta_{2}\\\\\n",
    "\\vdots\\\\\n",
    "\\beta_{p}\n",
    "\\end{bmatrix} \\to\n",
    "\\begin{bmatrix}\n",
    "\\beta_{1}\\\\\n",
    "\\beta_{2}\\\\\n",
    "\\vdots\\\\\n",
    "\\beta_{p}\\\\\n",
    "\\beta_{0}\n",
    "\\end{bmatrix}\\text{ where the dimensions change from p}\\times\\text{1}\\to\\text{(p+1)}\\times\\text{1}\n",
    "$\n",
    "\n",
    "#### Final shapes of each of the elements in the above equation\n",
    "\n",
    "* X $ \\rightarrow $ n$\\times$(p+$1$)\n",
    "\n",
    "* $\\beta\\to$(p+$1$)$\\times 1$\n",
    "\n",
    "* $\\hat{Y}\\to$n$\\times 1$\n",
    "\n",
    "<b>--> step 2</b>: Calculating the loss function\n",
    "\n",
    "We take the <b>Residual Mean Square Error</b> as the error calculating function\n",
    "\n",
    "Loss($\\beta$) = $ \\Sigma(Y - \\hat{Y}) $\n",
    "\n",
    "wkt.  $\\hat{Y} = X*\\beta$\n",
    "\n",
    "so substituting $\\hat{Y}$ in the Loss($\\beta$) we get\n",
    "\n",
    "Loss($\\beta$) = $(Y - X\\beta)^2 \\Rightarrow Loss(\\beta) = (Y - X\\beta)^T(Y - X\\beta)$\n",
    "\n",
    "<b>--> Step 3</b>: Calculating the <b>Gradient</b>:\n",
    "\n",
    "grad(Loss($\\beta$)) = $\\dfrac{dL}{d\\beta}$\n",
    "\n",
    "but if we directly derivate Loss function wrt. $\\beta$ we get $0$\n",
    "\n",
    "So we take partial derivatives where t = $(Y - X\\beta)$\n",
    "\n",
    "Then,\n",
    "\n",
    "grad(Loss($\\beta$)) = $\\dfrac{\\partial L}{\\partial t} \\times \\dfrac{\\partial t}{\\partial\\beta}\\approx [-2(Y - X\\beta)X^{T}]$\n",
    "\n",
    "as we know that the gradient must always be <u><b>zero</b></u>, so\n",
    "\n",
    "grad(Loss($\\beta$)) $\\approx -2(Y - X\\beta)X^{T} = 0$\n",
    "\n",
    "Therefore $\\Rightarrow Y = X\\beta$\n",
    "\n",
    "Then multiply both sides with $X^{T}$ on both sides to make X a Square matrix and then bringing $X^{T}X$ to the other side making it inverse\n",
    "\n",
    "$X^{T}Y = X^{T}X\\beta$\n",
    "\n",
    "$\\beta = (X^{T}X)^{-1}X^{T}Y$\n",
    "\n",
    "So finally we get $\\beta = (X^{T}X)^{-1}X^{T}Y$,. Which returns the parameters which can be used to fit in the equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression using <b>Gradient Descent</b>\n",
    "---\n",
    "\n",
    "Till the calculation of Loss function and it's gradient is same but the next steps will change<br>\n",
    "just substract the $(X^{T}*Loss)$ from the weights\n",
    "\n",
    "therefore updating the weights by,. wkt. $grad = -2(Y - X\\beta)X$\n",
    "\n",
    "$ \\Rightarrow grad \\approx 2*(\\hat{Y}-Y)*X $ as $Y = X*\\beta$\n",
    "\n",
    "for this equation we add a small value known as Learning Rate, which is written as<br>\n",
    "As we know the shapes of X and Y as $n\\times(p+1)$ and $n\\times1$ respectively<br> \n",
    "which is why we cannot directly multiply them as $Y\\times X$ since we cannot multiply $n\\times1$ with $n\\times(p+1)$<br>\n",
    "So we transpose X and then multiply with loss\n",
    "\n",
    "$ \\Rightarrow grad \\approx 2*lr*X^{T}*(\\hat{Y}-Y) $\n",
    "\n",
    "<b>This slowly updates the weights according to the error in the predictions with predefined weights, which is written as</b>\n",
    "\n",
    "$\\hat{w} = w - 2*lr*X^{T}*(\\hat{Y}-Y) $\n",
    "\n",
    "where,<br>\n",
    "$\\hat{w}\\rightarrow$ new or updated weights<br>\n",
    "$w\\rightarrow$ old weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
